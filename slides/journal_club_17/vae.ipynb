{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<big><big><big><big><big><big>Variational autoencoder</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<img style=\"float: right;\" src=\"gmum.png\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big>Sections</big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative vs. discriminative models in ML\n",
    "<img style=\"float: right;\" src=\"discriminative_vs_generative.png\" width=\"50%\">\n",
    "1. __discriminative__ compute _target_ values __only__ for __observable__ variables $p(c\\mid x)$\n",
    "  * e.g. logistic regression, SVM, maximum entropy, neural networks\n",
    "2. __generative__ find target __both__ for observable and _hidden_ variables\n",
    "  * hidden are somehow computed from observable\n",
    "  * model density distributions and __then__ conditional densities\n",
    "  * are able to generate __new__ data\n",
    "  * e.g. Gaussian mixture, Hidden Markov, probabilistic grammars, Naive Bayes, Latent Dirichlet, Restricted Boltzman, Generative Adversarial Networks GAN, VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need of generetive models\n",
    "they say that Richard Feynman said \"_What I cannot create, I do not understand_\"\n",
    "<img style=\"float: right;\" src=\"generative_models.png\" width=\"75%\">\n",
    "1. we have lots of data\n",
    "2. how to generate new data?\n",
    "  * draw a __code__\n",
    "  * train a model which would generate a __valid__ example of that code\n",
    "  * e.g. images, text, strings describing new _active) chemical molecules,...\n",
    "3. possible approaches\n",
    "  * __Generative Adversarial Networks GANs__\n",
    "    1. draw a _true_ image or a _generated_ one\n",
    "    2. __discriminator__ $Dis$ decide whether true or generated maximizing/minimizing \n",
    "    $$L_{GAN}=\\log(Dis(x))+\\log(1-Dis(Gen(z)))$$\n",
    "  * __Pixel RNN__\n",
    "    * autoregressive models\n",
    "    * conditional distributions of elements (pixels) on neighbouring ones\n",
    "  * __Variational AutoEncoders VAE__\n",
    "4. Generative models __should__ be able to learn features needed for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoders VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "<img style=\"float: right;\" src=\"vae_graphical_model.png\" width=\"35%\">\n",
    "\n",
    "$N$ examples $X=\\{x^{(i)}\\}_{i=1}^N$ generated by process\n",
    "  * $z^{(i)}$ generated from _prior_ distribution $p_{\\theta^\\ast}(z)$\n",
    "  * $x^{(i)}$ from _conditional_ distr. $p_{\\theta^\\ast}(x\\mid z)$\n",
    "\n",
    "\n",
    "1. true $\\theta^\\ast$ is unknown\n",
    "2. $z$ values are unknown \n",
    "\n",
    "\n",
    "Problems\n",
    "1. marginal likelihood\n",
    "$$p_\\theta(x)=\\int p_\\theta(z)\\,p_\\theta(x\\mid z)\\,dz$$ is _intractable_\n",
    "  * intractable posterior $$p_\\theta(z\\mid x)=\\frac{p_\\theta(x\\mid z)\\,p_\\theta(z)}{p_\\theta(x)}$$\n",
    "  and Expectation-Maximization EM algorithm cannot be used\n",
    "2. $N$ is large \n",
    "  * paramter updates need to be done on small mini-batches\n",
    "  * Monte Carlo MCMC would be too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Interested in\n",
    "<img style=\"float: right;\" src=\"vae_graphical_model.png\" width=\"35%\">\n",
    "1. __maximum likelihood MLE__ or __maximum a posteriori MAP__ _estimation_ of $\\theta$\n",
    "2. inference of _posterior_ $z$ given $x$\n",
    "  * efficient!\n",
    "  * useful for coding and/or representation tasks\n",
    "  * e.g. representation of discrete values in __continuous__ _latent space_\n",
    "3. inference of $x$\n",
    "  * image denoising, inpainting, resolution enhancement\n",
    "4. distribution of latent space\n",
    "  * rather in distribution of $p_\\theta(z\\mid x)$\n",
    "  * possible __traversal__ (__exploration__) of latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "<img style=\"float: right;\" src=\"vae_graphical_model.png\" width=\"35%\">\n",
    "1. definition of model $q_\\phi(z\\mid x)$\n",
    "  * approximation of _intractable_ $p_\\theta(z\\mid x)$\n",
    "  * parameters $\\theta$ and $\\phi$ trained jointly\n",
    "2. the whole model composed of\n",
    "  * $q_\\phi(z\\mid x)$ a probabilistic __encoder__\n",
    "    * given $x$ produces a __distribution__ over latent space!\n",
    "    * __posterior__ net\n",
    "  * $p_\\theta(x\\mid z)$ a probabilistic __decoder__\n",
    "    * given $z$ produces a __distribution__ of related $x$ values\n",
    "    * __likelihood__ net\n",
    "3. generating __new__ data from latent consists of \n",
    "  * __sampling__ $z$ latent space distribution \n",
    "  * then sampling from $p_\\theta(x\\mid z)$\n",
    "  * $q_\\phi(z\\mid x)$ and $p_\\theta(z\\mid x)$ should be identical, but \n",
    "  $$p_\\theta(z\\mid x)=\\frac{p_\\theta(x\\mid z)\\,p_\\theta(z)}{p_\\theta(x)}$$ is intractable\n",
    "4. solution: use __Kullback-Leibler__ divergence \n",
    "$$D_{KL}(p\\,\\rVert\\,q)=\\int_Xp(x)\\ln\\frac{p(x)}{q(x)}=\\int_Xp(x)\\ln\\,p(x)-p(x)\\ln\\,q(x)$$\n",
    "  * asymmetric, __not__ a distance, but\n",
    "    * set $p$\n",
    "    * ask whether $q$ approaches $p$\n",
    "    and approximate\n",
    "    $$D_{KL}(p\\,\\rVert\\,q)=\\int_X\\underbrace{p(x)\\ln\\,p(x)}_{const}-\\underbrace{p(x)}_{set}\\ln\\,q(x)=const-\\ln\\,q(x)$$\n",
    "5. Variational Autoencoder cost function\n",
    "$$L_{VAE}=D_{KL}(p_{DATA}\\,\\rVert\\,p_\\theta(x))+\\int_{x\\sim\\,p_{DATA}}D_{KL}(q_\\phi(z\\mid\\,x)\\,\\rVert\\,p_\\theta(z\\mid\\,x)),$$\n",
    "where \n",
    "  * $D_{KL}(p_{DATA}\\,\\rVert\\,p_\\theta(x))$ is the __reconstruction cost__\n",
    "  * $\\int_{x\\sim\\,p_{DATA}}D_{KL}(q_\\phi(z\\mid\\,x)\\,\\rVert\\,p_\\theta(z\\mid\\,x))$ is the __prior distribution cost__\n",
    "  * the latent space distribution $p(z)$ is usually $\\mathcal{N}(0, \\mathrm{I})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## VAE architecture\n",
    "<img src=\"vae-architecture.png\" width=\"80%\">\n",
    "1. posterior network $q_\\phi$\n",
    "  * computes the __parameters__ for the sampling from the latent space $z\\sim\\mathcal{N}(0,\\mathrm{I})$\n",
    "2. latent space\n",
    "  * sampling from the latent space\n",
    "3. likelihood network $p_\\theta$\n",
    "  * sampling from output (also $z\\sim\\mathcal{N}(0,\\mathrm{I})$)\n",
    "4. loss function\n",
    "  * $D_{KL}$ of the prior distribution vs $q_\\phi$\n",
    "  * recall loss as a negative likelihood -- cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import kl_divergence\n",
    "from tensorflow.contrib.distributions import MultivariateNormalDiag\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp_dim = 784\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, inp_dim], name=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder's layer size\n",
    "enc_dim = 100\n",
    "enc_w = tf.get_variable(\"enc_w\", shape=[inp_dim, enc_dim], dtype=tf.float32,\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "enc_b = tf.get_variable(\"enc_b\", shape=[enc_dim], dtype=tf.float32,\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "enc_hid = tf.nn.relu(tf.matmul(x, enc_w) + enc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# latent layer size\n",
    "z_dim = 200\n",
    "# mu and log sigma networks\n",
    "z_mu_w = tf.get_variable(\"z_mu_w\", shape=[enc_dim, z_dim], dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "z_mu_b = tf.get_variable(\"z_mu_b\", shape=[z_dim], dtype=tf.float32,\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "z_mu = tf.matmul(enc_hid, z_mu_w) + z_mu_b\n",
    "\n",
    "z_log_sigma_w = tf.get_variable(\"z_log_sigma_w\", shape=[enc_dim, z_dim], \n",
    "                                dtype=tf.float32, \n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "z_log_sigma_b = tf.get_variable(\"z_log_sigma_b\", shape=[z_dim], dtype=tf.float32,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "# clip log(sigma)\n",
    "z_log_sigma = tf.maximum(tf.matmul(enc_hid, z_log_sigma_w) + z_log_sigma_b, -3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample from latent space z\n",
    "mvn = MultivariateNormalDiag(loc=z_mu, scale_diag=tf.exp(z_log_sigma), name=\"mvn\")\n",
    "z_sample = mvn.sample(name=\"z_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decoder layer size\n",
    "dec_dim = 100\n",
    "dec_w = tf.get_variable(\"dec_w\", shape=[z_dim, dec_dim], dtype=tf.float32,\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "dec_b = tf.get_variable(\"dec_b\", shape=[dec_dim], dtype=tf.float32,\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "dec_hid = tf.nn.sigmoid(tf.nn.relu(tf.matmul(z_sample, dec_w) + dec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mu and log sigma for decoder sampling\n",
    "dec_mu_w = tf.get_variable(\"dec_mu_w\", shape=[dec_dim, inp_dim], dtype=tf.float32, \n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "dec_mu_b = tf.get_variable(\"dec_mu_b\", shape=[inp_dim], dtype=tf.float32,\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "dec_mu = tf.matmul(dec_hid, dec_mu_w) + dec_mu_b\n",
    "\n",
    "dec_log_sigma_w = tf.get_variable(\"dec_log_sigma_w\", shape=[dec_dim, inp_dim], \n",
    "                                  dtype=tf.float32, \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "dec_log_sigma_b = tf.get_variable(\"dec_log_sigma_b\", shape=[inp_dim], \n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "# clip log(sigma)\n",
    "dec_log_sigma = tf.maximum(\n",
    "                    tf.matmul(dec_hid, dec_log_sigma_w) + dec_log_sigma_b, -3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample log_prob from output\n",
    "p_theta_x_dist = MultivariateNormalDiag(loc=dec_mu, \n",
    "                                        scale_diag=tf.exp(dec_log_sigma), \n",
    "                                        name=\"p_theta_x_dist\")\n",
    "p_theta_x = tf.reduce_mean(p_theta_x_dist.log_prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kullback-Leibler divergence\n",
    "post_distribution = mvn\n",
    "prior_distribution = MultivariateNormalDiag(tf.zeros(z_dim),\n",
    "                                            scale_diag=tf.ones(z_dim))\n",
    "kl_loss = tf.divide(\n",
    "    tf.reduce_mean(kl_divergence(post_distribution, prior_distribution)),\n",
    "    float(z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# negative cross entropy\n",
    "cross_ent = -p_theta_x\n",
    "# cross_ent = -tf.reduce_mean(-tf.reduce_sum(x * tf.log(-p_theta_x), \n",
    "#                                          reduction_indices=[1]))\n",
    "cross_ent = tf.divide(tf.reduce_mean(cross_ent), float(inp_dim))\n",
    "\n",
    "# loss function\n",
    "# loss_func = inp_dim * kl_loss + z_dim * cross_ent\n",
    "loss_func = z_dim * kl_loss + inp_dim * cross_ent\n",
    "\n",
    "# optimizer\n",
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(loss_func)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1:  +828.22321 < (cr +1.03229, D_KL +0.09455)\t[0s]\n",
      "  2500:  -919.36005 < (cr -1.19686, D_KL +0.09488)\t[235s]\n",
      "  5000:  -992.02509 < (cr -1.29638, D_KL +0.12169)\t[233s]\n",
      "  7500: -1029.88867 < (cr -1.34847, D_KL +0.13655)\t[223s]\n",
      " 10000: -1056.85059 < (cr -1.38515, D_KL +0.14553)\t[221s]\n",
      " 12500: -1058.08643 < (cr -1.38956, D_KL +0.15665)\t[222s]\n",
      " 15000: -1072.38159 < (cr -1.40908, D_KL +0.16168)\t[221s]\n",
      " 17500: -1077.74231 < (cr -1.41716, D_KL +0.16655)\t[222s]\n",
      " 20000: -1090.44849 < (cr -1.43468, D_KL +0.17170)\t[222s]\n",
      " 22500: -1083.87842 < (cr -1.42722, D_KL +0.17532)\t[221s]\n",
      " 25000: -1091.44714 < (cr -1.43739, D_KL +0.17734)\t[222s]\n",
      " 27500: -1108.40796 < (cr -1.45946, D_KL +0.17904)\t[222s]\n",
      " 30000: -1108.21216 < (cr -1.45919, D_KL +0.17897)\t[227s]\n",
      " 32500: -1106.80237 < (cr -1.45797, D_KL +0.18122)\t[221s]\n",
      " 35000: -1109.13440 < (cr -1.46135, D_KL +0.18280)\t[223s]\n",
      " 37500: -1111.41016 < (cr -1.46453, D_KL +0.18390)\t[222s]\n",
      " 40000: -1112.04785 < (cr -1.46570, D_KL +0.18530)\t[222s]\n",
      " 42500: -1105.60583 < (cr -1.45746, D_KL +0.18522)\t[222s]\n",
      " 45000: -1126.34619 < (cr -1.48415, D_KL +0.18615)\t[224s]\n",
      " 47500: -1121.42444 < (cr -1.47808, D_KL +0.18696)\t[222s]\n",
      " 50000: -1110.26196 < (cr -1.46426, D_KL +0.18859)\t[222s]\n",
      " 52500: -1108.32056 < (cr -1.46220, D_KL +0.19022)\t[222s]\n",
      " 55000: -1110.75464 < (cr -1.46474, D_KL +0.18801)\t[222s]\n",
      " 57500: -1121.69214 < (cr -1.47874, D_KL +0.18819)\t[223s]\n",
      " 60000: -1136.83997 < (cr -1.49771, D_KL +0.18684)\t[233s]\n",
      " 62500: -1126.12646 < (cr -1.48458, D_KL +0.18891)\t[223s]\n",
      " 65000: -1119.33362 < (cr -1.47572, D_KL +0.18817)\t[223s]\n",
      " 67500: -1134.80713 < (cr -1.49584, D_KL +0.18966)\t[223s]\n",
      " 70000: -1118.80457 < (cr -1.47544, D_KL +0.18969)\t[223s]\n",
      " 72500: -1123.13477 < (cr -1.48134, D_KL +0.19116)\t[222s]\n",
      " 75000: -1121.61292 < (cr -1.47930, D_KL +0.19078)\t[224s]\n",
      " 77500: -1118.97766 < (cr -1.47593, D_KL +0.19074)\t[223s]\n",
      " 80000: -1123.72681 < (cr -1.48183, D_KL +0.19016)\t[222s]\n",
      " 82500: -1124.02161 < (cr -1.48267, D_KL +0.19195)\t[222s]\n",
      " 85000: -1141.20605 < (cr -1.50352, D_KL +0.18778)\t[223s]\n",
      " 87500: -1135.11414 < (cr -1.49671, D_KL +0.19153)\t[223s]\n",
      " 90000: -1129.71753 < (cr -1.48991, D_KL +0.19185)\t[222s]\n",
      " 92500: -1132.43469 < (cr -1.49294, D_KL +0.19013)\t[222s]\n",
      " 95000: -1115.38501 < (cr -1.47172, D_KL +0.19220)\t[222s]\n",
      " 97500: -1135.08875 < (cr -1.49618, D_KL +0.18959)\t[229s]\n",
      "100000: -1130.02551 < (cr -1.49049, D_KL +0.19261)\t[222s]\n",
      "102500: -1135.57104 < (cr -1.49757, D_KL +0.19264)\t[222s]\n",
      "105000: -1129.42224 < (cr -1.48988, D_KL +0.19323)\t[228s]\n",
      "107500: -1143.35168 < (cr -1.50692, D_KL +0.19036)\t[224s]\n",
      "110000: -1135.09497 < (cr -1.49639, D_KL +0.19036)\t[222s]\n",
      "112500: -1131.62415 < (cr -1.49197, D_KL +0.19042)\t[223s]\n",
      "115000: -1131.25525 < (cr -1.49222, D_KL +0.19322)\t[225s]\n",
      "117500: -1134.72253 < (cr -1.49682, D_KL +0.19393)\t[221s]\n",
      "120000: -1133.68311 < (cr -1.49486, D_KL +0.19143)\t[221s]\n",
      "122500: -1137.97839 < (cr -1.50053, D_KL +0.19219)\t[228s]\n",
      "125000: -1136.43982 < (cr -1.49854, D_KL +0.19206)\t[224s]\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "glob_init = tf.global_variables_initializer()\n",
    "loops = 125000\n",
    "batch_size = 512\n",
    "interval = 2500\n",
    "\n",
    "session.run(glob_init)\n",
    "s = time()\n",
    "for k in range(loops):\n",
    "    data, _ = mnist.train.next_batch(batch_size=batch_size)\n",
    "    _, ls_fn, cr_ent, kl_ls = session.run([optimizer, loss_func, cross_ent, kl_loss],\n",
    "                                          feed_dict = {x: data})\n",
    "    if k == 0 or (k + 1) % interval == 0:\n",
    "        t = time()\n",
    "        print(\"{:6d}: {:+11.5f} < (cr {:+8.5f}, D_KL {:+8.5f})\\t[{:.0f}s]\".format(\n",
    "            k + 1, ls_fn, cr_ent, kl_ls, (t - s)))\n",
    "    s = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_grid(arr):\n",
    "    return np.concatenate([np.concatenate(row, axis=1) for row in arr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "digit_shape = (28, 28)\n",
    "digits, _ = mnist.train.next_batch(batch_size=batch_size)\n",
    "inp, out = session.run([x, dec_mu], feed_dict={x: digits})\n",
    "arr = np.stack((inp, out)) * 255\n",
    "arr = arr.reshape(arr.shape[:2] + digit_shape)\n",
    "img = Image.fromarray(draw_grid(arr))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear interpolation\n",
    "steps = 14\n",
    "#digits, _ = mnist.train.next_batch(batch_size=batch_size)\n",
    "z_values = session.run([z_sample], feed_dict={x: digits})[0]\n",
    "zzz = np.array([]).reshape((0, z_values.shape[-1]))\n",
    "for k in range(batch_size - 1):\n",
    "    z_from = z_values[k]\n",
    "    z_to = z_values[k + 1]\n",
    "    this_zzz = np.stack([p * z_from + (1 - p) * z_to \n",
    "                         for p in np.linspace(1, 0, steps)])\n",
    "    zzz = np.vstack((zzz, this_zzz))\n",
    "arr = session.run([dec_mu], feed_dict={z_sample: zzz})[0]\n",
    "arr[arr < 0] = 0\n",
    "arr[arr > 1] = 1\n",
    "arr = arr.reshape((-1, steps) + digit_shape)\n",
    "img = Image.fromarray(draw_grid(255 * arr))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more polar interpolation\n",
    "steps = 14\n",
    "#digits, _ = mnist.train.next_batch(batch_size=batch_size)\n",
    "z_values = session.run([z_sample], feed_dict={x: digits})[0]\n",
    "zzz = np.array([]).reshape((0, z_values.shape[-1]))\n",
    "for k in range(batch_size - 1):\n",
    "    z_from = z_values[k]\n",
    "    z_to = z_values[k + 1]\n",
    "    this_zzz = np.stack([np.sqrt(p) * z_from + np.sqrt((1 - p)) * z_to \n",
    "                        for p in np.linspace(1, 0, steps)])\n",
    "#    this_zzz = np.stack([(np.cos(p * np.pi / 2) * z_from + np.sin(p * np.pi / 2) * z_to) \n",
    "#                         for p in np.linspace(0, 1, steps)])\n",
    "    zzz = np.vstack((zzz, this_zzz))\n",
    "arr = session.run([dec_mu], feed_dict={z_sample: zzz})[0]\n",
    "arr[arr < 0] = 0\n",
    "arr[arr > 1] = 1\n",
    "arr = arr.reshape((-1, steps) + digit_shape)\n",
    "img = Image.fromarray(255 * draw_grid(arr))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications?\n",
    "1. generation of new data\n",
    "2. learning of features from an extended data set\n",
    "3. generation of data fulfilling __with__ some attributes\n",
    "4. organization of the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE in search of automata\n",
    "<img style=\"float: right;\" src=\"automaton.png\" width=\"45%\">\n",
    "__Synchronizing automaton__ if there exists a word that is to leave the automaton in one particular _whatever_ state it started at.\n",
    "\n",
    "__Cerny conjecture:__ the shortest reset word for a synchronizing automaton has a length at most $(n-1)^2$ (1964)\n",
    "  * finding the shortest reset word is an NP problem\n",
    "  * still an open problem!\n",
    "  * with number of states $n$ growing the probability of an automaton being synchronizable approaches $1$\n",
    "  * it is very hard to find an __extreme__ automaton, i.e. one with reset word of $(n-1)^2$ length\n",
    "  * only a few except are known for larger $n$\n",
    "  \n",
    "Is it possible to find some extreme or almost-extreme automata using ML methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE architecture used\n",
    "<img src=\"vae-for-automata.png\" width=\"100%\">\n",
    "\n",
    "1. 1-D convolution layers in the posterior (encoder) network\n",
    "  * automata represented serialized\n",
    "    * with higher number of states and larger alphabet it may be hard\n",
    "2. latent space with $\\mathcal{N}(0,\\mathrm{I})$ distribution\n",
    "  * last layer of the posterior network has 2 linear fully connected networks\n",
    "    * approximation of $\\mu$ mean value of $z$\n",
    "    * approximation of $\\log\\,\\sigma$\n",
    "  * $z$ is __sampled__ from the given distribution\n",
    "3. likelihood network\n",
    "  * a series of recurrent networks (GRU here, LSTM's might be used as well)\n",
    "  * input word is set as __repeated__ value of $z$\n",
    "  * values are processed in sequence modifying the memory state\n",
    "4. output value is decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding and interpolation\n",
    "<img src=\"linear-interpolation-automata-1.png\" width=\"110%\">\n",
    "1. interpolation through the latent space\n",
    "  * take two automata\n",
    "  * find (sample) their $z$ images\n",
    "  * interpolate linearly between them\n",
    "  * some results (or after small predefined modifications) represent valid automata\n",
    "  * several new data __do not__ come from training set -- a generative model!\n",
    "2. But...\n",
    "<img style=\"float: right;\" src=\"latent-distance-from-mean.png\" width=\"65%\">\n",
    "  * histogram of distances of points sampled from $\\mathcal{N}(0,\\mathrm{I})$ latent space\n",
    "  * the distance from the center of $$z=X_1^2+\\dots+X_d^2$$ follows a $\\chi^2(d)$ distribution\n",
    "    * mean value $\\mathbf{E}\\|z\\|^2=d$\n",
    "    * variance $Var\\left[\\frac{\\|z\\|^2}{d}\\right]=\\frac{2}{d}$\n",
    "    * $\\chi^2(d)$ is a special type of _gamma_ distribution $\\Gamma(\\frac{d}{2}, 2)$\n",
    "    * for $d>30$ it is very similar to $\\mathcal{N}(\\sqrt{2d-1}, 1)$\n",
    "  * the high dimensional $\\mathcal{N}(0,\\mathrm{I})$ distribution is actually a sphere - it _looks_ like a __soap bubble__\n",
    "    * in high dimensions most data in a ball are actually in a very thin skin of it\n",
    "    * true not only for gaussian distribution\n",
    "    * two randomly chosen $z_1$ and $z_2$ have virtually the same norm\n",
    "    * linear interpolation $a*z_1+(1-a)*z_2$ has norm $\\sqrt{a^2+(1+a)^2}<1$\n",
    "  * linear interpolation necessesarily passes through an empty space...\n",
    "  * better a polar coordinates interpolation $$\\sqrt{a}\\,z_1+\\sqrt{1-a}\\,z_2$$\n",
    "  going through the sphere\n",
    "3. an encode-decode interpolation\n",
    "  * start with linear\n",
    "  * decode strings __and__ encode them again $k$ times\n",
    "  \n",
    "<img src=\"linear-interpolation-automata-2.png\" width=\"110%\">\n",
    "  * more results represent valid automata\n",
    "  * there are more valid automata not from the training set\n",
    "  \n",
    "4. other possibilieties\n",
    "  * change the prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for a manifold\n",
    "1. a well known phenomenon is that when a problem is given in $K$ dimensional space, it is generally placed on a manifold of a __much lower dimension__\n",
    "  * several dimensions are actually spurious\n",
    "    * dimensionality reduction methods\n",
    "  * __manifold learning__ adresses this by trying to identify this mamifold and learn __there__\n",
    "    * e.g. swiss-roll structure\n",
    "2. problem: find a manifold where the data are placed so that it would be possible to perform simple __arithmetical__ operations which modify attributes\n",
    "\n",
    "<img src=\"automata-manifold.png\" width=\"110%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "1. VAEs return _fuzzy_ images (and data)\n",
    "  * problem of distribution $\\mathcal{N}(\\mu,\\sigma)$ returned\n",
    "  * vary small modifications of the prior space\n",
    "  * cooperation with GANs\n",
    "  * other distributions of the prior space\n",
    "    * but the bubble effect is true not only for the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Some literature\n",
    "1. Kingma, Welling, _Auto encoding variational Bayes_, arXiV:1312.6114, stat.ML, 2014, __basic paper describing the training of the model__\n",
    "2. Gomez-Bombarelli, Duvenaud _et al_, _Automatic chemical design using a data-driven continuous representation of molecules_, arXiV:1610.02415, cs.LG, 2015, __use of VAE in modelling drug-like discrete small molecules for the search of new active ones__\n",
    "3. Larsen, Sonderby, Larochelle, Winther, _Autoencoding beyond pixels using a learned similarity metric_, arXiV:1512.09300, cs.LG, February 2016, __a composition of VAE together with a GAN; exploration of latent space with attributes__\n",
    "4. Rezende, Mohamed, _Variational inference with normalizing flows\", arXiV:1505.05770, stat.ML, 2016, __modification of posterior distributions with a series of invertible transformations__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
